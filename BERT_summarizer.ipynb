{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "BERT\n",
        "\n",
        "Extractive Summarization"
      ],
      "metadata": {
        "id": "mYnylY6WX7yx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bert-extractive-summarizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBTsBP_5YMQr",
        "outputId": "02d0180d-1866-4ba0-af5d-4f70e886735c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: bert-extractive-summarizer in /usr/local/lib/python3.7/dist-packages (0.10.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (from bert-extractive-summarizer) (3.4.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from bert-extractive-summarizer) (1.0.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (from bert-extractive-summarizer) (4.24.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->bert-extractive-summarizer) (1.2.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->bert-extractive-summarizer) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->bert-extractive-summarizer) (1.21.6)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->bert-extractive-summarizer) (3.1.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (3.0.10)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (1.10.2)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (1.0.3)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (3.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (21.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (0.10.1)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (0.4.2)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (0.6.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (2.4.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (3.0.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (2.0.8)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (2.11.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (2.0.7)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (4.64.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (57.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (1.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (8.1.5)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy->bert-extractive-summarizer) (3.10.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy->bert-extractive-summarizer) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy->bert-extractive-summarizer) (5.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (1.24.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy->bert-extractive-summarizer) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy->bert-extractive-summarizer) (0.0.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy->bert-extractive-summarizer) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy->bert-extractive-summarizer) (2.0.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (0.10.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (2022.6.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (0.13.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (4.13.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (3.8.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnO87fF_X6IF",
        "outputId": "8aa9a2c9-57ab-4424-dac6-683930c58118"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "from summarizer import Summarizer\n",
        "model=Summarizer()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "pIuTKEqmdM8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv(\"/content/papers1 - Sheet1.csv\")"
      ],
      "metadata": {
        "id": "L6Tvqi_OdD8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "Ea0fB9Ycdq8M",
        "outputId": "479ee2e9-8c02-4897-b35c-971209740f23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               TITLE  \\\n",
              "0  Stock Prediction using Machine Learning a Revi...   \n",
              "1  Sentiment Analysis for Product Recommendation ...   \n",
              "2  EDM – survey of performance factors and algori...   \n",
              "3                            Fake News Detector: FND   \n",
              "4  Effect of image binarization thresholds on bre...   \n",
              "\n",
              "                                        INTRODUCTION  \\\n",
              "0  Machine learning can be defined as the data wh...   \n",
              "1  Sentimental Analysis is nothing but the task o...   \n",
              "2  Data mining is a process to extract informatio...   \n",
              "3  Fake news can be defined as a pseudo-news or j...   \n",
              "4  Early detection of breast cancer plays a very ...   \n",
              "\n",
              "                                               Ref 1  \\\n",
              "0  In machine learning, data is obtained by knowl...   \n",
              "1  Sentiment analysis is computational methodolog...   \n",
              "2  Educational Data Mining refers to techniques, ...   \n",
              "3  Parameters including websites data, website va...   \n",
              "4                                                NaN   \n",
              "\n",
              "                                               Ref 2  \\\n",
              "0  Machine learning is used to describe the proce...   \n",
              "1  This paper presents a computational approach f...   \n",
              "2  Data Mining is the extraction of information f...   \n",
              "3  Rapid spread of fake news is a serious problem...   \n",
              "4                                                NaN   \n",
              "\n",
              "                                               Ref 3  \n",
              "0  Machine learning can be defined as the data wh...  \n",
              "1  Sentimental Analysis is a task of Natural Lang...  \n",
              "2  Data mining is a process to extracting informa...  \n",
              "3  Fake news, defined as a pseudo-news or junk ne...  \n",
              "4                                                NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-69462fd8-20e9-4d99-97cc-aaf0893e6542\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TITLE</th>\n",
              "      <th>INTRODUCTION</th>\n",
              "      <th>Ref 1</th>\n",
              "      <th>Ref 2</th>\n",
              "      <th>Ref 3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Stock Prediction using Machine Learning a Revi...</td>\n",
              "      <td>Machine learning can be defined as the data wh...</td>\n",
              "      <td>In machine learning, data is obtained by knowl...</td>\n",
              "      <td>Machine learning is used to describe the proce...</td>\n",
              "      <td>Machine learning can be defined as the data wh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Sentiment Analysis for Product Recommendation ...</td>\n",
              "      <td>Sentimental Analysis is nothing but the task o...</td>\n",
              "      <td>Sentiment analysis is computational methodolog...</td>\n",
              "      <td>This paper presents a computational approach f...</td>\n",
              "      <td>Sentimental Analysis is a task of Natural Lang...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>EDM – survey of performance factors and algori...</td>\n",
              "      <td>Data mining is a process to extract informatio...</td>\n",
              "      <td>Educational Data Mining refers to techniques, ...</td>\n",
              "      <td>Data Mining is the extraction of information f...</td>\n",
              "      <td>Data mining is a process to extracting informa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Fake News Detector: FND</td>\n",
              "      <td>Fake news can be defined as a pseudo-news or j...</td>\n",
              "      <td>Parameters including websites data, website va...</td>\n",
              "      <td>Rapid spread of fake news is a serious problem...</td>\n",
              "      <td>Fake news, defined as a pseudo-news or junk ne...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Effect of image binarization thresholds on bre...</td>\n",
              "      <td>Early detection of breast cancer plays a very ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-69462fd8-20e9-4d99-97cc-aaf0893e6542')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-69462fd8-20e9-4d99-97cc-aaf0893e6542 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-69462fd8-20e9-4d99-97cc-aaf0893e6542');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5wVFddCpKSR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text1=df['INTRODUCTION'][0]\n",
        "text2=df['INTRODUCTION'][1]\n",
        "text3=df['INTRODUCTION'][2]\n",
        "text4=df['INTRODUCTION'][3]\n"
      ],
      "metadata": {
        "id": "3B-byrbQ-jaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reference11=df['Ref 1'][0]\n",
        "reference21=df['Ref 1'][1]\n",
        "reference31=df['Ref 1'][2]\n",
        "reference41=df['Ref 1'][3]\n",
        "#print(reference11, reference21, reference31, reference41)"
      ],
      "metadata": {
        "id": "iLIPACEa-jYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reference12=df['Ref 2'][0]\n",
        "reference22=df['Ref 2'][1]\n",
        "reference32=df['Ref 2'][2]\n",
        "reference42=df['Ref 2'][3]"
      ],
      "metadata": {
        "id": "lGXXpJnS-jd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WgB9hqtqKSVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reference13=df['Ref 3'][0]\n",
        "reference23=df['Ref 3'][1]\n",
        "reference33=df['Ref 3'][2]\n",
        "reference43=df['Ref 3'][3]"
      ],
      "metadata": {
        "id": "9Nc384u5_io4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu\n"
      ],
      "metadata": {
        "id": "9XUVYMyiKSYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sample 1"
      ],
      "metadata": {
        "id": "sdAAFcKE9Enm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text1 =df[\"INTRODUCTION\"][0]"
      ],
      "metadata": {
        "id": "j8xxrdDRYIH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "ZeUMoRQ3dvlg",
        "outputId": "1748c310-2009-4536-8b0b-24573d188279"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Machine learning can be defined as the data which is obtained by knowledge extraction. Machines don‟t have to be programmed explicitly instead they are trained to make decisions that are driven by data. Instead of writing a code for every specific problem, data is provided to the generic algorithms and logic is developed on the basis of that data. When a machine improves its performance based on its past experiences it can be said that machine has truly learnt. The technique for most accurate prediction is by learning from past instances, and to make a program to do this is best possible with machine learning techniques. Any machine learning technique (supervised or unsupervised) is efficient enough to generate rules for programs, in consideration with present ones to take a better decision. In this scenario, the decision is whether the stock will increase or decrease (Stock analysis).'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_text1 = text1.strip().replace(\"\\n\",\"\")\n"
      ],
      "metadata": {
        "id": "eyxssyweYcVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary1=model(preprocess_text1)\n",
        "print(summary1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzaVJeJtYekI",
        "outputId": "aa5ea775-ecd6-4e45-dd64-244debe29402"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Machine learning can be defined as the data which is obtained by knowledge extraction. Instead of writing a code for every specific problem, data is provided to the generic algorithms and logic is developed on the basis of that data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PByFaPaHYyCA",
        "outputId": "54a65628-5e5f-4d12-ecce-337a20f035b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from rouge) (1.15.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge import Rouge"
      ],
      "metadata": {
        "id": "OPU4Z9ZFY1GG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reference1 =\"In machine learning, data is obtained by knowledge extraction. Machine learning techniques learn from past predictions and give an outcome. \"\n",
        "rouge = Rouge()\n",
        "rouge.get_scores(summary1, reference1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2m7EQmYYmS7",
        "outputId": "02cabdec-46ad-4493-e751-c0891a6bdf7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'rouge-1': {'r': 0.42857142857142855,\n",
              "   'p': 0.2727272727272727,\n",
              "   'f': 0.3333333285802469},\n",
              "  'rouge-2': {'r': 0.3, 'p': 0.15384615384615385, 'f': 0.20338982602700384},\n",
              "  'rouge-l': {'r': 0.42857142857142855,\n",
              "   'p': 0.2727272727272727,\n",
              "   'f': 0.3333333285802469}}]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zZUnSKnod1HS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1YfAnQTeq4D",
        "outputId": "fb1145af-c346-4415-ee38-91c9ba0ed0e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU SCORE: 1.0\n"
          ]
        }
      ],
      "source": [
        "reference_1 = [\n",
        "    reference12.split(),\n",
        "    reference13.split()\n",
        "]\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "summary_1=summary1.split()\n",
        "score = sentence_bleu(reference_1, summary_1)\n",
        "print(\"BLEU SCORE:\",score)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sample 2"
      ],
      "metadata": {
        "id": "XVuQB7sq9BWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#sample 2:Sentiment Analysis for Product Recommendation Using Random Forest\n",
        "text2 "
      ],
      "metadata": {
        "id": "5gJ6aI7pYwtg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "6cb6bfab-14ff-4df4-db2f-de1b54148d56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Sentimental Analysis is nothing but the task of Natural Language Processing. It observes the attitude of customer behind the comments. Sentiment analysis is a method of identifying sentiments in text.  Sentiment analysis is computational methodology of extracting sentiments from text, speech or dataset. It can classify emotions, attitude, opinion and subjective impression into polarity. Researchers and decision makers understand the approach of the people using consumer sentiment analysis and can make decisions accordingly. Business analysis application can be developed by using this technique. Social network development and popularity is increasing day by day. A growing number of users prefer to order online products and prefer to share their experiences on social networks. Searching for appropriate product online is a difficult task. Recommender system can help users by providing suggestions. Recommender system creates recommendation list. There are three Recommendation system approaches, content based, collaborative and hybrid approach. The content-based approach considers the information of an item and the user’s profile and the recommendation of items is based on the user’s preferences. The collaborative-based approach analyses the user behaviour and preferences and find the same preferences among people. It is well known that collaborative-based techniques are normally more accurate than content-based techniques. The hybrid approach combines both methods. User’s feedback is important tool in recommendation system. The recommendation systems gives suggestion based on the user history and on the user’s profile, but nowadays, the recommendations are starting to explore making more suggestions based on sentiment analysis.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_text2 = text2.strip().replace(\"\\n\",\"\")\n"
      ],
      "metadata": {
        "id": "He8Gh9In6XTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary2=model(preprocess_text2)\n",
        "print(summary2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEBHdw4d6Xuj",
        "outputId": "e658c3b9-2773-4c82-ac92-81d0c9484f6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentimental Analysis is nothing but the task of Natural Language Processing. Sentiment analysis is computational methodology of extracting sentiments from text, speech or dataset. Business analysis application can be developed by using this technique. The content-based approach considers the information of an item and the user’s profile and the recommendation of items is based on the user’s preferences.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reference2 =\"Identifying breast cancer at early stages using mammographic images and classify between mass and normal breast tissues, through Image binarization methods. \"\n",
        "rouge = Rouge()\n",
        "rouge.get_scores(summary2, reference2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YazMuKby6dVR",
        "outputId": "eca4c257-6ba2-428c-88a6-c9f51c6e7be0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'rouge-1': {'r': 0.1, 'p': 0.043478260869565216, 'f': 0.06060605638200213},\n",
              "  'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0},\n",
              "  'rouge-l': {'r': 0.1, 'p': 0.043478260869565216, 'f': 0.06060605638200213}}]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dLkor_z3J5Fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2b1aaa8-dcb0-4695-a8c6-c4aa73f53b55",
        "id": "Uh07cJGTJ5bS"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU SCORE: 0.8104494292952378\n"
          ]
        }
      ],
      "source": [
        "reference_2 = [\n",
        "    reference22.split(),\n",
        "    reference23.split()\n",
        "]\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "summary_2=summary2.split()\n",
        "score = sentence_bleu(reference_2, summary_2)\n",
        "print(\"BLEU SCORE:\",score)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sample 3"
      ],
      "metadata": {
        "id": "hXg1M71p9KqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#sample 3 : EDM\n",
        "text3 "
      ],
      "metadata": {
        "id": "Q2zyTmKQ9Jvq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "5b84d380-8f62-4af9-8cf6-bba3f8121510"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Data mining is a process to extract information from a data set and transform it into an understandable structure for further use. Educational Data Mining (EDM) relates to the inter-disciplinary research that deals with the development of various methods and techniques to explore the data generated from different educational sources. Analysing educational data could provide information of student’s behaviours, based on which education policies would be made properly.[1] Educational Data Mining refers to techniques, tools, and research designed for automatically extracting meaning from large repositories of data generated by or related to people’s learning activities in educational settings. Recent advances in educational technology, including the increase in computing power and the ability to log fine-grained data about student’s use of a computer-based learning environment, have led to an increased interest in developing techniques for analysing the large amounts of data generated in educational settings. This paper presents a study on current state of EDM and identifies the algorithms applied. Section 2 talks about the Goals and Methods in EDM, section 3 proposes purpose of study and presents the study in summarized fashion. Section 4 talks about role and study about performance factors. Section 5 presents the findings from the survey followed by conclusion. '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_text3 = text3.strip().replace(\"\\n\",\"\")\n"
      ],
      "metadata": {
        "id": "uDQZc3yC959r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary3=model(preprocess_text3)\n",
        "print(summary3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fHI26WX96Dj",
        "outputId": "a04d26f1-6528-4b8d-d35c-12821ed1550d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data mining is a process to extract information from a data set and transform it into an understandable structure for further use. Educational Data Mining (EDM) relates to the inter-disciplinary research that deals with the development of various methods and techniques to explore the data generated from different educational sources.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reference3 =\"Sentiment analysis is computational methodology of ex-tracting sentiments  from text, speech or dataset to classify emotions, attitude, opinion and subjective impression into polarity.\"\n",
        "rouge = Rouge()\n",
        "rouge.get_scores(summary2, reference3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eaAc4QU96KE",
        "outputId": "b27f5956-00c7-4bf9-ebf5-9487d35f4ebc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'rouge-1': {'r': 0.5652173913043478,\n",
              "   'p': 0.2826086956521739,\n",
              "   'f': 0.3768115897584541},\n",
              "  'rouge-2': {'r': 0.45454545454545453,\n",
              "   'p': 0.18181818181818182,\n",
              "   'f': 0.25974025565862713},\n",
              "  'rouge-l': {'r': 0.5652173913043478,\n",
              "   'p': 0.2826086956521739,\n",
              "   'f': 0.3768115897584541}}]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reference_3 = [\n",
        "    reference32.split(),\n",
        "    reference33.split()\n",
        "]\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "summary_3=summary3.split()\n",
        "score = sentence_bleu(reference_3, summary_3)\n",
        "print(\"BLEU SCORE:\",score)"
      ],
      "metadata": {
        "id": "6TQKAZAq96P-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26782f41-abfa-4123-f2cf-cc7fc0c706c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU SCORE: 0.5693042932883674\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ag_GgOev96VY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sample 4"
      ],
      "metadata": {
        "id": "qKUgGSIZ9wTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#sample 4 :Fake News"
      ],
      "metadata": {
        "id": "3MeUabqh9xyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_text4 = text4.strip().replace(\"\\n\",\"\")\n"
      ],
      "metadata": {
        "id": "PDZZJdVy968e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary4=model(preprocess_text4)\n",
        "print(summary4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxAFlhrK97B7",
        "outputId": "76c07e0c-d294-48a5-af5a-fbb06585bb00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fake news can be defined as a pseudo-news or junk news which is false information or content spread via different broadcasting media. This rapid spread of fake news is a serious problem calling for AI solutions.[3] To overcome this, the system is developed which identifies the news authenticity and identifies it as Fake News or Real News. This is done by analyzing the news content where analysis is done on the basis of defined parameters.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reference4 =\"Educational Data Mining refers to techniques, tools, and research designed for automatically extracting meaning from large repositories of data generated from different educational sources, analysing which could predict student behaviour. \"\n",
        "rouge = Rouge()\n",
        "rouge.get_scores(summary4, reference4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0A5CLIb97Fm",
        "outputId": "d99924b0-d5c2-4694-d360-ecb607f3185a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'rouge-1': {'r': 0.16666666666666666,\n",
              "   'p': 0.09803921568627451,\n",
              "   'f': 0.12345678545953377},\n",
              "  'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0},\n",
              "  'rouge-l': {'r': 0.06666666666666667,\n",
              "   'p': 0.0392156862745098,\n",
              "   'f': 0.04938271138545997}}]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reference_4 = [\n",
        "    reference42.split(),\n",
        "    reference43.split()\n",
        "]\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "summary_4=summary4.split()\n",
        "score = sentence_bleu(reference_4, summary_4)\n",
        "print(\"BLEU SCORE:\",score)"
      ],
      "metadata": {
        "id": "PaqQE_Cz97Ji",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93fd5f51-bead-4396-bc9b-a0613f990d61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU SCORE: 0.7986312884738124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GLP1XHF997Mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sample 5"
      ],
      "metadata": {
        "id": "G-gM0aoK9ySk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Sample 5: A review of microscopic analysis of blood cells for disease detection with AI perspective\n",
        "text5 = \"\"\"\n",
        "Blood, the most integral part of the body, is constituted of white blood cells (WBC), red blood cells (RBC), platelets, and plasma. \n",
        "This can be further categorized as; cells and platelets are about 45% of human blood, whereas the remaining 55% is filled by plasma (the yellow fluid in the blood). \n",
        "These components and their physical properties like size, shape, color, and count in the whole blood change due to ingress of any foreign object or micro-organism can lead to any sort of infections.\n",
        "\n",
        "There are different pathological procedures for the detection of diseases. In most cases, microscopic imaging plays a vital role in predicting and detecting abnormalities and occurrences of diseases within the body. \n",
        "Typically, the health of any person is judged by analyzing different features of blood cells and their counts.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "SA6q_K959zfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_text5= text5.strip().replace(\"\\n\",\"\")\n"
      ],
      "metadata": {
        "id": "jzq1i4bO97nj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary5=model(preprocess_text5)\n",
        "print(summary5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7XQ4K1097qk",
        "outputId": "7075cb1e-47e8-443c-82d5-616af2ca4040"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Blood, the most integral part of the body, is constituted of white blood cells (WBC), red blood cells (RBC), platelets, and plasma. Typically, the health of any person is judged by analyzing different features of blood cells and their counts.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reference5 =\"There are different pathological procedures for the detection of diseases. Typically, the health of any person is judged by analyzing different features of blood cells and their counts. In most cases, microscopic imaging plays a vital role in predicting and detecting abnormalities and occurrences of diseases within the body. \"\n",
        "rouge = Rouge()\n",
        "rouge.get_scores(summary5, reference5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obnh5mxQ97tS",
        "outputId": "5539d609-13f5-47e2-d7bd-498e1d7f3397"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'rouge-1': {'r': 0.43902439024390244,\n",
              "   'p': 0.6206896551724138,\n",
              "   'f': 0.5142857094326531},\n",
              "  'rouge-2': {'r': 0.3541666666666667,\n",
              "   'p': 0.4594594594594595,\n",
              "   'f': 0.3999999950837371},\n",
              "  'rouge-l': {'r': 0.43902439024390244,\n",
              "   'p': 0.6206896551724138,\n",
              "   'f': 0.5142857094326531}}]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sS-MBuEu97wG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rQICnUyJ9z8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sample 6"
      ],
      "metadata": {
        "id": "W87xzxnU90bf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Sample 6: On Context Awareness for Multisensor Data Fusion in IoT\n",
        "text6 = \"\"\"\n",
        "The concept of the internet of things (IoT) originated in the Auto-ID Center at the Massachusetts Institute of Technology in 1999.\n",
        "Kevin Ashton had imagined a world in which all electronic devices are networked and every object, whether physical or electronic, is electronically tagged with information applicable to that object.\n",
        "The underlying aim of this concept is the achievement of pervasive connections between the internet and objects around us. \n",
        "It is perfect assimilation of real-world objects with logical things.\n",
        "Multi-sensor data fusion system is analogous to human who can sense the environment with the help of their sensory organs like nose, ears, skin, etc. and make correct inferences about their surroundings.\n",
        "Multisensor data fusion refers to the comprehensive fusing of sensory data from multiple sensors and related information in order to provide more reliable and accurate information that could be achieved using a single, independent sensor.\n",
        "Sensor fusion technology was primarily developed for Military surveillance research and robotics by US DoD. \n",
        "Later, it has got commercially wider acceptance in the areas, such as intelligent transport system, geographic information, land and ocean surveillance, robotics, data and information security, medical surveillance, diagnosis, etc.\n",
        " The only way to gain the required amount of information with the expected intelligence is viable with the help of multisensors data fusion approach.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "jBIDBGxY91kE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_text6 = text6.strip().replace(\"\\n\",\"\")"
      ],
      "metadata": {
        "id": "SFtByMSl98W-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary6=model(preprocess_text6)\n",
        "print(summary6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cg71GgzF98bX",
        "outputId": "36d0ff77-8622-425d-f97b-8657a31f6f09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The concept of the internet of things (IoT) originated in the Auto-ID Center at the Massachusetts Institute of Technology in 1999.Kevin Ashton had imagined a world in which all electronic devices are networked and every object, whether physical or electronic, is electronically tagged with information applicable to that object. The only way to gain the required amount of information with the expected intelligence is viable with the help of multisensors data fusion approach.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reference6 =\"The aim of the internet of things is to achieve pervasive connections between the internet and objects around us. Multisensor data fusion refers to the comprehensive fusing of sensory data from multiple sensors. Sensor fusion technology was primarily developed for Military surveillance research and robotics by US DoD. \"\n",
        "rouge = Rouge()\n",
        "rouge.get_scores(summary6, reference6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKK6GBc_98d9",
        "outputId": "2ac43716-ff10-4c29-fa1f-290a5e0e0b32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'rouge-1': {'r': 0.25, 'p': 0.17543859649122806, 'f': 0.20618556216388575},\n",
              "  'rouge-2': {'r': 0.10638297872340426,\n",
              "   'p': 0.06944444444444445,\n",
              "   'f': 0.08403360866605493},\n",
              "  'rouge-l': {'r': 0.25, 'p': 0.17543859649122806, 'f': 0.20618556216388575}}]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HIcjCJ__98gb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EVlC2SVU98il"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sample 7"
      ],
      "metadata": {
        "id": "8oNqhttP92qv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#sample 7: Fake News Detector: FND\n",
        "\n",
        "text7='''\n",
        "Fake news can be defined as a pseudo-news or junk news which is false information or content spread via different broadcasting media.\n",
        "This false information is mainly created to mislead people or to cause ravage to a person, organization, etc., or to gain financial benefits. \n",
        "Easier ways to get news nowadays are from sources like media outlets, newspapers, journalists where they follow defined strict codes of practice; this is the traditional method. \n",
        "But the increase in internet network has changed the certain aspect of publishing and sharing the news and information with less editorial standards and regulations.\n",
        "On the other hand, the news is being manipulated by various networking sites based on personal opinions or interests.\n",
        "Fake news causes immense damage in different ways such as misleading people with sharing false information, this can create racists ideas or can damage people’s sentiments, which can give rise to violence among people i.e. causing real-life impacts, etc. \n",
        "This rapid spread of fake news is a serious problem calling for AI solutions. To overcome this, the system is developed which identifies the news authenticity and identifies it as Fake News or Real News. \n",
        "This is done by analyzing the news content where analysis is done on the basis of defined parameters. \n",
        "In the proposed system the user directly enters the data or the news content (news headlines) and enters into the systems search bar and gets the result. \n",
        "The proposed system works upon the two main algorithms i.e. Naive Bayes algorithm and TFIDF algorithm. The defined parameters help to identify the news content and increases the accuracy of the system. \n",
        "Parameters considered are website data, website validates, the similarity of content, timestamps, reviews, and grammatical analysis. \n",
        "These parameters calculate the news authenticity and generate the final result as fake or real news.\n",
        "'''"
      ],
      "metadata": {
        "id": "olYUspKb930h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_text7 = text7.strip().replace(\"\\n\",\"\")"
      ],
      "metadata": {
        "id": "K2a4iFT299bR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary7=model(preprocess_text7)\n",
        "print(summary7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxNuDE6Y99d9",
        "outputId": "80da57dd-d4d1-47bb-afc7-c5689ab700be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fake news can be defined as a pseudo-news or junk news which is false information or content spread via different broadcasting media. On the other hand, the news is being manipulated by various networking sites based on personal opinions or interests. The defined parameters help to identify the news content and increases the accuracy of the system.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reference7 =\"Parameters including websites data, website validates, similarity of content, timestamps, reviews, and grammarical analysis calculate the authenticity of the news using the Naive Bayes and TFIDF algorithms that increase accuracy of the system.\"\n",
        "rouge = Rouge()\n",
        "rouge.get_scores(summary7, reference7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEAfGzSR99g7",
        "outputId": "458272d9-fb32-4f0f-ecdb-ca0588044a3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'rouge-1': {'r': 0.2222222222222222,\n",
              "   'p': 0.13333333333333333,\n",
              "   'f': 0.1666666619791668},\n",
              "  'rouge-2': {'r': 0.12903225806451613,\n",
              "   'p': 0.07272727272727272,\n",
              "   'f': 0.0930232512033534},\n",
              "  'rouge-l': {'r': 0.2222222222222222,\n",
              "   'p': 0.13333333333333333,\n",
              "   'f': 0.1666666619791668}}]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R9ODjrP599jS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c5mEbtCl99mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sample 8"
      ],
      "metadata": {
        "id": "3FbUTlKi94L1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Sample 8: Iris Liveness Detection for Biometric Authentication: A Systematic Literature Review and Future Directions\n",
        "\n",
        "text8='''\n",
        "During the primeval eras, there were restricted choices and ways for personal iden- tification. \n",
        "Nowadays, we have an era of computer vision and biometrics, which does not involve any external artifact or token to recognize others. \n",
        "Instead, individuals can be acknowledged with their own biological or behavioral features with the aid of biometrics as an alternative to their associations, possessions, or any secret information.\n",
        "The necessity of mechanized and precise identification directed us to biometrics, which controls technology to accelerate the course of human identification and authentication. \n",
        "The biometric ID has been originated and replaced the printed IDs. This allows you to verify your identity, deprived of carrying any card or document (www.bayometric.com (accessed on 4 August 2021)). \n",
        "The authentication is a vital stage for offering admittance to the resources to the approved individuals. \n",
        "Conventional authentication systems such as a pin, card, and password cannot differentiate between real users and imposters who have fraudulently accessed the system.\n",
        " There are many possibilities of forgetting the password/pin or stealing and misplacement of the card. The device that allows the automatic identification of an individual is known as a biometric system. \n",
        " The biometric authentication system is easy to use, and there is no need to remember a password, card, and pin code.\n",
        "Biometrics have been extensively discovered for their automation, approachability, and accuracy with the mounting security needs of our everyday lives. \n",
        "It is a mechanized device that studies human beings’ physiological and behavioral features for their unique classification as the technology has differentiated from detection to criminal identifications and forensics.\n",
        "There are several diverse markets of biometric technology which exist today. Most of the markets appear to be mounting swiftly. \n",
        "The global biometric technologies market is anticipated to reach 19.08 billion US dollars in 2021, while the contactless biometric technologies market is predicted to grow to over 30.15 billion US dollars by 2027 (www. statista.com (accessed on 4 August 2021)).\n",
        " Biometrics have been successfully deployed in a variety of applications where security is of primary concern. \n",
        " \n",
        " For example, airport check-in and check-out personal identification cards [3]; sensitive information from unauthorized users, and credit card authentication.\n",
        "Several biometric characteristics such as the fingerprint, iris, palm print, and the face, are used for authentication and recognition.\n",
        " As compared to fingerprint and face, the iris-based authentication provides stronger contactless identification of the user. \n",
        " Table 1 displays various applications domains for iris biometric detection. The contactless approach helps to prevent the spread of viruses and diseases such as COVID-19. \n",
        " Iris has complex textures and unique features, so it is widely used in identifying and authentication the person in many applications. \n",
        " Aadhar project uses the biometric system to identify the citizens of India, Amsterdam airport, and the US Canadian border. Even though the iris has a unique texture pattern, there is a possibility of spoofing by the imposter. People attack the biometric device to obtain the rights of others.\n",
        "Iris detection systems can be easily spoofed by using different types of contact lens such as transparent lenses, colored lenses, and textured lenses.\n",
        " By using the transparent lenses, the fraudster cannot alter the iris texture but can modify the reflection property of the Iris recognition system. \n",
        " An imposter can conceal the real texture of an Iris with the aid of textured color lenses. The system can also be rapidly spoofed by replaying a video as well as a print attack. \n",
        " This means the iris pattern is acquainted with the machine by printing an iris image. Print attacks are performed in two modes:. \n",
        " First is print and scan, in which the high-quality printed iris pattern is scanned, and second is print and capture, in which the scanner takes the snapshot. \n",
        " Due to vulnerabilities in traditional security systems that lead to frequent security breaches, biometrics is increasingly becoming important.\n",
        "  Thus, ILD for biometric authentication is a significant research area.\n",
        "'''"
      ],
      "metadata": {
        "id": "VPC69cNU95Pc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_text8 = text8.strip().replace(\"\\n\",\"\")"
      ],
      "metadata": {
        "id": "5EmDrWy09-UK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary8=model(preprocess_text8)\n",
        "print(summary8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1q2x-RXC9-Xh",
        "outputId": "ddd447a8-ecc9-49ef-b503-f16d66406f1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "During the primeval eras, there were restricted choices and ways for personal iden- tification. The device that allows the automatic identification of an individual is known as a biometric system. The biometric authentication system is easy to use, and there is no need to remember a password, card, and pin code. There are several diverse markets of biometric technology which exist today. As compared to fingerprint and face, the iris-based authentication provides stronger contactless identification of the user. By using the transparent lenses, the fraudster cannot alter the iris texture but can modify the reflection property of the Iris recognition system. This means the iris pattern is acquainted with the machine by printing an iris image. Print attacks are performed in two modes:.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reference8 =\"Biometrics is a mechanized device that studies human beings' physiological and behavioral features for their unique classification as the technology has differentiated from detection to criminal identifications and forensics. The biometric authentication system is easy to use, and there is no need to remember a password, card, and pin code.\"\n",
        "rouge = Rouge()\n",
        "rouge.get_scores(summary8, reference8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22vX8u9n9-aR",
        "outputId": "fa4d56c1-a32a-4517-b603-d83ca848ffba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'rouge-1': {'r': 0.5714285714285714,\n",
              "   'p': 0.2696629213483146,\n",
              "   'f': 0.36641220938406854},\n",
              "  'rouge-2': {'r': 0.42857142857142855,\n",
              "   'p': 0.17647058823529413,\n",
              "   'f': 0.2499999958680556},\n",
              "  'rouge-l': {'r': 0.5238095238095238,\n",
              "   'p': 0.24719101123595505,\n",
              "   'f': 0.3358778582390304}}]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle"
      ],
      "metadata": {
        "id": "B9E4TiJr9-dU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename='bert.sav'\n",
        "pickle.dump(model, open(filename, 'wb'))"
      ],
      "metadata": {
        "id": "sV54YnIK9-fr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "atbUUMLid8-A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}